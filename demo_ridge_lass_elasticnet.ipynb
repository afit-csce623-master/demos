{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ridge Lasso Grid Search Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOa/NqRu2wtvgeJpbY7dJkX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afit-csce623-master/demos/blob/main/demo_ridge_lass_elasticnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqRv7gnYXNHF"
      },
      "source": [
        "# Ridge, Lasso, and ElasticNet Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUcs5fH98-Lu"
      },
      "source": [
        "This notebook demonstrates the use of Ridge, Lasso, and ElasticNet on a linear regression dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8dnwK5nXSr2"
      },
      "source": [
        "## Generate Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ykyhqHJ-xYc"
      },
      "source": [
        "Here we generate regression data with 8 features, but only 3 features are needed to determine the target `y`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3XoZQtRM6hl"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "from IPython.display import Markdown as md\n",
        "from IPython.display import display, Math, Latex\n",
        "\n",
        "# initialize random state and regression parameters\n",
        "random_state = np.random.RandomState(101)\n",
        "n_features = 8\n",
        "n_informative = 3\n",
        "bias_data = random_state.uniform(0,10)\n",
        "noise = 2\n",
        "\n",
        "# Build a regression task\n",
        "X, y, coef_data = make_regression(n_samples=6000, \n",
        "                                  n_features=n_features, n_informative=n_informative, \n",
        "                                  bias=bias_data, noise=noise,\n",
        "                                  random_state=random_state, coef=True)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATRKQBA3XUkd"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQJREDcY-52E"
      },
      "source": [
        "The following helper functions will be used throughout this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz4DvDr2Xhw8"
      },
      "source": [
        "### Generate Model Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wffxOZIE-9SL"
      },
      "source": [
        "This function generates a model string of the form $y = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n$ where $n$ is the number of coefficients in `betas`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOZbGERCOnlE"
      },
      "source": [
        "def generate_model_function(beta0, betas, feature_names):\n",
        "    if np.isclose(beta0, 0):\n",
        "        model_function = f'$y = '\n",
        "    else:\n",
        "        model_function = f'$y = {beta0:.3f} + '\n",
        "\n",
        "    for beta, feature_name in zip(betas, feature_names):\n",
        "        model_function += f'{beta:.3f}{feature_name} + '\n",
        "    \n",
        "    return model_function[:-3] + '$'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GIccjVpXlli"
      },
      "source": [
        "### Clean Up Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taJNjfp7_bMA"
      },
      "source": [
        "Some models don't clean-up very small coefficients, leaving values looking like $ \\dots + 0.000 x_3 + \\dots $. This function will filter out coefficients that are near zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBC7nnL4Qg6R"
      },
      "source": [
        "def clean_up_model(coefs, names):\n",
        "    non_zero_coef_filter = np.logical_not(np.isclose(coefs, 0))\n",
        "    non_zero_coefs = coefs[non_zero_coef_filter]\n",
        "    non_zero_names = names[non_zero_coef_filter]\n",
        "\n",
        "    return non_zero_coefs, non_zero_names\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j8f2or6Xp5T"
      },
      "source": [
        "### Generate Clean Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbcGUnDb_y7n"
      },
      "source": [
        "This function combines the above two functions to return a cleaned-up model string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNEoRvPhf1EL"
      },
      "source": [
        "def generate_clean_model(beta0, coefs, names):\n",
        "    coefs_, names_ = clean_up_model(coefs, names)\n",
        "    return generate_model_function(beta0, coefs_, names_)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ato9lwsaXs-_"
      },
      "source": [
        "### Powerset Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RarJVNi1_27H"
      },
      "source": [
        "This function generates the power set of a list, excluding the empty set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMnCU3E2rmQw"
      },
      "source": [
        "# powerset function\n",
        "\n",
        "from itertools import chain, combinations\n",
        "\n",
        "def powerset(iterable):\n",
        "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
        "    s = list(iterable)\n",
        "\n",
        "    # start with 1, as we don't need the empty set\n",
        "    return chain.from_iterable(combinations(s, r) for r in range(1, len(s)+1)) \n",
        "\n",
        "feature_powerset = list(powerset(range(n_features)))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tmXjkZyAAqn"
      },
      "source": [
        "## Prepatory actons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6OBz3oiAIma"
      },
      "source": [
        "Here, we split the train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuphFhLhNZfd"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split and sequester test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=random_state)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTjhNsAWAOMM"
      },
      "source": [
        "We set some variables that we will reuse throught the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoEsu_RLNRgt"
      },
      "source": [
        "kfold_size = 10\n",
        "\n",
        "# create feature names -- these are generic feature names x_1, x_2, ..., x_25\n",
        "feature_names = np.array([f'x_{{{idx+1}}}' for idx in range(n_features)])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58IQ3r_9TExT"
      },
      "source": [
        "## Ridge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mYuY_ayYxpo"
      },
      "source": [
        "### Set alpha values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aclnC-OAIum5"
      },
      "source": [
        "We will set alpha values. With Ridge regression, we will include 0.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvJ5dcFJYzD3"
      },
      "source": [
        "alpha_min = 0.0000001\n",
        "alpha_max = 100000\n",
        "alpha_num = int(abs(np.log10(alpha_min)) + abs(np.log10(alpha_max)) + 1)\n",
        "alphas = np.geomspace(alpha_min, alpha_max, num=alpha_num)\n",
        "alphas = np.insert(alphas, 0, 0)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCrdAxa0Yhry"
      },
      "source": [
        "### Ridge - manual cross validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk2x6oTDI46y"
      },
      "source": [
        "The following code uses the Ridge regression model for each alpha value. We keep track of each RMSE, and select the model that yielded the lowest RMSE. We compare the model with the model that generated the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "ObNmaKPxWQBs",
        "outputId": "5c81dd81-4089-421f-f195-a4af8e04a163"
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "df_ridge = pd.DataFrame(columns=['alpha', 'rmse', 'r2'])\n",
        "\n",
        "for alpha in alphas:\n",
        "    ridge = Ridge(alpha=alpha, fit_intercept=True, random_state=random_state)\n",
        "    scores = cross_val_score(ridge, X_train, y_train, scoring='neg_root_mean_squared_error', cv=kfold_size)\n",
        "    ridge.fit(X_train, y_train)\n",
        "    df_ridge.loc[len(df_ridge.index)] = [alpha, -np.mean(scores), ridge.score(X_test, y_test)]\n",
        "\n",
        "best_ridge = {}\n",
        "best_ridge['alpha'] = df_ridge.loc[np.argmin(df_ridge['rmse'])]['alpha']\n",
        "best_ridge['model'] = Ridge(alpha=best_ridge['alpha'], fit_intercept=True, random_state=random_state)\n",
        "best_ridge['model'].fit(X_train, y_train)\n",
        "best_ridge['r2'] = best_ridge['model'].score(X_test, y_test)\n",
        "y_pred = best_ridge['model'].predict(X_test)\n",
        "best_ridge['rmse'] = mean_squared_error(y_test, y_pred, squared=False)\n",
        "\n",
        "display(md(f'Best Ridge with $\\\\alpha$={best_ridge[\"alpha\"]}: ' +\n",
        "           f'{(generate_clean_model(best_ridge[\"model\"].intercept_, best_ridge[\"model\"].coef_, feature_names))}, ' +\n",
        "           f'RMSE: {best_ridge[\"rmse\"]:.4f}, $R^2$: {best_ridge[\"r2\"]:.4f}'))\n",
        "display(md(f'Original Data: {(generate_clean_model(bias_data, coef_data, feature_names))}'))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Best Ridge with $\\alpha$=0.0: $y = 5.190 + 0.004x_{1} + 39.521x_{2} + -0.001x_{3} + 16.122x_{4} + 34.609x_{5} + 0.015x_{6} + -0.001x_{7} + -0.023x_{8}$, RMSE: 2.0544, $R^2$: 0.9987",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Original Data: $y = 5.164 + 39.530x_{2} + 16.115x_{4} + 34.584x_{5}$",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdQgNT4wYlDo"
      },
      "source": [
        "### RidgeCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0CclZFwJNKV"
      },
      "source": [
        "The following code uses the RidgeCV to automatically find the alpha value that yields the smallest RMSE. We compare the model with the model that generated the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "id": "bWx9570fYm-E",
        "outputId": "5142c82b-7388-4724-a770-ebc7c9e2509d"
      },
      "source": [
        "from sklearn.linear_model import RidgeCV\n",
        "\n",
        "ridgeCV = RidgeCV(alphas=alphas,\n",
        "                  scoring='neg_root_mean_squared_error',\n",
        "                  cv=kfold_size)\n",
        "ridgeCV.fit(X_train, y_train)\n",
        "ridgeCV_r2 = ridgeCV.score(X_test, y_test)\n",
        "y_pred = ridgeCV.predict(X_test)\n",
        "ridgeCV_rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "display(md(f'RidgeCV with $\\\\alpha$={ridgeCV.alpha_}: ' +\n",
        "           f'{(generate_clean_model(ridgeCV.intercept_, ridgeCV.coef_, feature_names))}, ' +\n",
        "           f'RMSE: {ridgeCV_rmse:.4f}, $R^2$: {ridgeCV_r2:.4f}'))\n",
        "display(md(f'Original Data: {(generate_clean_model(bias_data, coef_data, feature_names))}'))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "RidgeCV with $\\alpha$=0.0: $y = 5.190 + 0.004x_{1} + 39.521x_{2} + -0.001x_{3} + 16.122x_{4} + 34.609x_{5} + 0.015x_{6} + -0.001x_{7} + -0.023x_{8}$, RMSE: 2.0544, $R^2$: 0.9987",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Original Data: $y = 5.164 + 39.530x_{2} + 16.115x_{4} + 34.584x_{5}$",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGVfIW4ubks2"
      },
      "source": [
        "### Ridge with subsets of features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N75D307tJWaR"
      },
      "source": [
        "Neither Ridge nor RidgeCV is able to fully eliminate unimportant features. Here, we test the power set of features to find the set of features and alpha value that generates the lowest RMSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "eiP13kfDbnF_",
        "outputId": "d0d25c78-1e90-4abe-bfe8-9e00803f2976"
      },
      "source": [
        "# use power set\n",
        "\n",
        "df_ridge = pd.DataFrame(columns=['feature_set', 'alpha', 'rmse', 'r2'])\n",
        "\n",
        "for feature_set in feature_powerset:\n",
        "    for alpha in alphas:\n",
        "        ridge = Ridge(alpha=alpha, fit_intercept=True, random_state=random_state)\n",
        "        scores = cross_val_score(ridge, X_train[:,np.array(feature_set)], y_train, scoring='neg_root_mean_squared_error', cv=10)\n",
        "        ridge.fit(X_train[:,np.array(feature_set)], y_train)\n",
        "        df_ridge.loc[len(df_ridge.index)] = [','.join(str(e) for e in feature_set), alpha, -np.mean(scores), ridge.score(X_test[:,np.array(feature_set)], y_test)]\n",
        "\n",
        "best_ridge = {}\n",
        "best_ridge['params'] = df_ridge.loc[np.argmin(df_ridge['rmse'])]\n",
        "best_ridge['features'] = [int(e) for e in best_ridge['params']['feature_set'].split(',')]\n",
        "best_ridge['feature_names'] = feature_names[np.array(best_ridge['features'])]\n",
        "best_ridge['alpha'] = best_ridge['params']['alpha']\n",
        "\n",
        "best_ridge['model'] = Ridge(alpha=best_ridge['alpha'])\n",
        "best_ridge['model'].fit(X_train[:,np.array(best_ridge['features'])], y_train)\n",
        "best_ridge['r2'] = best_ridge['model'].score(X_test[:,np.array(best_ridge['features'])], y_test)\n",
        "y_pred = best_ridge['model'].predict(X_test[:,np.array(best_ridge['features'])])\n",
        "best_ridge['rmse'] = mean_squared_error(y_test, y_pred, squared=False)\n",
        "\n",
        "display(md(f'Best Ridge with $\\\\alpha$={best_ridge[\"alpha\"]}: ' +\n",
        "           f'{(generate_clean_model(best_ridge[\"model\"].intercept_, best_ridge[\"model\"].coef_, best_ridge[\"feature_names\"]))}, ' + \n",
        "           f'RMSE: {best_ridge[\"rmse\"]:.4f}, ' +\n",
        "           f'$R^2$: {best_ridge[\"r2\"]:.4f}'\n",
        "           ))\n",
        "display(md(f'Original Data: {(generate_clean_model(bias_data, coef_data, feature_names))}'))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Best Ridge with $\\alpha$=0.0: $y = 5.190 + 39.521x_{2} + 16.122x_{4} + 34.609x_{5}$, RMSE: 2.0540, $R^2$: 0.9987",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Original Data: $y = 5.164 + 39.530x_{2} + 16.115x_{4} + 34.584x_{5}$",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81tbIKecZErl"
      },
      "source": [
        "## Lasso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qihdwBP9ZHV_"
      },
      "source": [
        "### Set alpha values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjTE1niELOI_"
      },
      "source": [
        "We will set alpha values. With Lasso regression, we will not include 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fltmQvKZWBl"
      },
      "source": [
        "alpha_min = 0.0000001\n",
        "alpha_max = 100000\n",
        "alpha_num = int(abs(np.log10(alpha_min)) + abs(np.log10(alpha_max)) + 1)\n",
        "alphas = np.geomspace(alpha_min, alpha_max, num=alpha_num)\n",
        "\n",
        "# Lasso is unreliable with alpha=0, use LinearRegression instead when alpha=0 is desired\n",
        "#alphas = np.insert(alphas, 0, 0)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFgXQuwfZLlm"
      },
      "source": [
        "### Lasso - manual cross validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0gTkM7iLSl-"
      },
      "source": [
        "The following code uses the Lasso regression model for each alpha value. We keep track of each RMSE, and select the model that yielded the lowest RMSE. We compare the model with the model that generated the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "jka9W-bxZf-V",
        "outputId": "32d8c115-907e-46ec-d5d4-4aed7c0c5ba8"
      },
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "df_lasso = pd.DataFrame(columns=['alpha', 'rmse', 'r2'])\n",
        "\n",
        "for alpha in alphas:\n",
        "    lasso = Lasso(alpha=alpha, fit_intercept=True, random_state=random_state)\n",
        "    scores = cross_val_score(lasso, X_train, y_train, scoring='neg_root_mean_squared_error', cv=kfold_size)\n",
        "    lasso.fit(X_train, y_train)\n",
        "    df_lasso.loc[len(df_lasso.index)] = [alpha, -np.mean(scores), lasso.score(X_test, y_test)]\n",
        "\n",
        "best_lasso = {}\n",
        "best_lasso['alpha'] = df_lasso.loc[np.argmin(df_lasso['rmse'])]['alpha']\n",
        "best_lasso['model'] = Lasso(alpha=best_lasso['alpha'], fit_intercept=True, random_state=random_state)\n",
        "best_lasso['model'].fit(X_train, y_train)\n",
        "best_lasso['r2'] = best_lasso['model'].score(X_test, y_test)\n",
        "y_pred = best_lasso['model'].predict(X_test)\n",
        "best_lasso['rmse'] = mean_squared_error(y_test, y_pred, squared=False)\n",
        "\n",
        "display(md(f'Best Ridge with $\\\\alpha$={best_lasso[\"alpha\"]}: ' +\n",
        "           f'{(generate_clean_model(best_lasso[\"model\"].intercept_, best_lasso[\"model\"].coef_, feature_names))}, ' +\n",
        "           f'RMSE: {best_lasso[\"rmse\"]:.4f}, $R^2$: {best_lasso[\"r2\"]:.4f}'))\n",
        "display(md(f'Original Data: {(generate_clean_model(bias_data, coef_data, feature_names))}'))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Best Ridge with $\\alpha$=0.01: $y = 5.190 + 39.511x_{2} + 16.112x_{4} + 34.599x_{5} + 0.005x_{6} + -0.012x_{8}$, RMSE: 2.0544, $R^2$: 0.9987",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Original Data: $y = 5.164 + 39.530x_{2} + 16.115x_{4} + 34.584x_{5}$",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwVSU7dEZOhF"
      },
      "source": [
        "### LassoCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMVm5z1SLYwY"
      },
      "source": [
        "The following code uses the LassoCV to automatically find the alpha value that yields the smallest RMSE. We compare the model with the model that generated the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "-UP8k_u3ZGWE",
        "outputId": "24a37f64-c410-4302-bc26-97df5be37a63"
      },
      "source": [
        "from sklearn.linear_model import LassoCV\n",
        "\n",
        "lassoCV = LassoCV(alphas=alphas,\n",
        "                #   scoring='neg_root_mean_squared_error',\n",
        "                  cv=kfold_size)\n",
        "lassoCV.fit(X_train, y_train)\n",
        "lassoCV_r2 = lassoCV.score(X_test, y_test)\n",
        "y_pred = lassoCV.predict(X_test)\n",
        "lassoCV_rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "display(md(f'LassoCV with $\\\\alpha$={lassoCV.alpha_}: ' +\n",
        "           f'{(generate_clean_model(lassoCV.intercept_, lassoCV.coef_, feature_names))}, ' +\n",
        "           f'RMSE: {lassoCV_rmse:.4f}, $R^2$: {lassoCV_r2:.4f}'))\n",
        "display(md(f'Original Data: {(generate_clean_model(bias_data, coef_data, feature_names))}'))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "RidgeCV with $\\alpha$=0.01: $y = 5.190 + 39.511x_{2} + 16.112x_{4} + 34.599x_{5} + 0.005x_{6} + -0.012x_{8}$, RMSE: 2.0544, $R^2$: 0.9987",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Original Data: $y = 5.164 + 39.530x_{2} + 16.115x_{4} + 34.584x_{5}$",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZobiGFXhcIVi"
      },
      "source": [
        "### Lasso with subsets of features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnkfxZvPLnGd"
      },
      "source": [
        "Though it performs better than Ridge, neither Lasso nor LassoCV is able to fully eliminate unimportant features. Here, we test the power set of features to find the set of features and alpha value that generates the lowest RMSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "RG91g4QsDrdl",
        "outputId": "185cab53-350a-46af-b6b1-fc434cbcb0d9"
      },
      "source": [
        "# use power set\n",
        "\n",
        "df_lasso = pd.DataFrame(columns=['feature_set', 'alpha', 'rmse', 'r2'])\n",
        "\n",
        "for feature_set in feature_powerset:\n",
        "    for alpha in alphas:\n",
        "        lasso = Lasso(alpha=alpha, fit_intercept=True, random_state=random_state)\n",
        "        scores = cross_val_score(lasso, X_train[:,np.array(feature_set)], y_train, scoring='neg_root_mean_squared_error', cv=10)\n",
        "        lasso.fit(X_train[:,np.array(feature_set)], y_train)\n",
        "        df_lasso.loc[len(df_lasso.index)] = [','.join(str(e) for e in feature_set), alpha, -np.mean(scores), lasso.score(X_test[:,np.array(feature_set)], y_test)]\n",
        "\n",
        "best_lasso = {}\n",
        "best_lasso['params'] = df_lasso.loc[np.argmin(df_lasso['rmse'])]\n",
        "best_lasso['features'] = [int(e) for e in best_lasso['params']['feature_set'].split(',')]\n",
        "best_lasso['feature_names'] = feature_names[np.array(best_lasso['features'])]\n",
        "best_lasso['alpha'] = best_lasso['params']['alpha']\n",
        "\n",
        "best_lasso['model'] = Lasso(alpha=best_lasso['alpha'])\n",
        "best_lasso['model'].fit(X_train[:,np.array(best_lasso['features'])], y_train)\n",
        "best_lasso['r2'] = best_lasso['model'].score(X_test[:,np.array(best_lasso['features'])], y_test)\n",
        "y_pred = best_lasso['model'].predict(X_test[:,np.array(best_lasso['features'])])\n",
        "best_lasso['rmse'] = mean_squared_error(y_test, y_pred, squared=False)\n",
        "\n",
        "display(md(f'Best Lasso with $\\\\alpha$={best_lasso[\"alpha\"]}: ' +\n",
        "           f'{(generate_clean_model(best_lasso[\"model\"].intercept_, best_lasso[\"model\"].coef_, best_lasso[\"feature_names\"]))}, ' + \n",
        "           f'RMSE: {best_lasso[\"rmse\"]:.4f}, ' +\n",
        "           f'$R^2$: {best_lasso[\"r2\"]:.4f}'\n",
        "           ))\n",
        "display(md(f'Original Data: {(generate_clean_model(bias_data, coef_data, feature_names))}'))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Best Lasso with $\\alpha$=0.0001: $y = 5.190 + 39.521x_{2} + 16.122x_{4} + 34.609x_{5}$, RMSE: 2.0540, $R^2$: 0.9987",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Original Data: $y = 5.164 + 39.530x_{2} + 16.115x_{4} + 34.584x_{5}$",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7zE5eoVeDzX"
      },
      "source": [
        "## ElasticNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b6kgVtaeaLT"
      },
      "source": [
        "### Set alpha values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNeSzHUTLw4I"
      },
      "source": [
        "We will set alpha values. With ElasticNet regression, which includes Lasso when l1_ratio=1, we will not include 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nUnghLXeb7e"
      },
      "source": [
        "alpha_min = 0.0000001\n",
        "alpha_max = 100000\n",
        "alpha_num = int(abs(np.log10(alpha_min)) + abs(np.log10(alpha_max)) + 1)\n",
        "alphas = np.geomspace(alpha_min, alpha_max, num=alpha_num)\n",
        "\n",
        "# Lasso is unreliable with alpha=0, use LinearRegression instead when alpha=0 is desired\n",
        "#alphas = np.insert(alphas, 0, 0)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU6kNhXwekAb"
      },
      "source": [
        "### Set lr_ratio values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuDJQsPhME9R"
      },
      "source": [
        "Set l1_ratio values. This is that amount of l1 regularization the linear model will attempt to perform (l1_ratio = 1.0 corresponds with Lasso, and l1_ratio = 0.0 corresponds with Ridge)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyT63444eGle"
      },
      "source": [
        "l1_ratios = np.linspace(0.01, 1.0 ,num=100)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ztjUcEvjwTx"
      },
      "source": [
        "### ElasticNetCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlK4GqaGMDh2"
      },
      "source": [
        "The following code uses the ElasticNetCV to automatically find the alpha and l1_ratio values that yield the smallest RMSE. We compare the model with the model that generated the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "K7nDCe_Vf29r",
        "outputId": "fb838c3b-123b-4c0b-84b0-b6f5fda219c2"
      },
      "source": [
        "from sklearn.linear_model import ElasticNetCV\n",
        "\n",
        "elastic_netCV = ElasticNetCV(alphas=alphas,\n",
        "                      l1_ratio = l1_ratios,\n",
        "                #   scoring='neg_root_mean_squared_error',\n",
        "                  cv=kfold_size,\n",
        "                  max_iter=1000)\n",
        "elastic_netCV.fit(X_train, y_train)\n",
        "elastic_netCV_r2 = elastic_netCV.score(X_test, y_test)\n",
        "y_pred = elastic_netCV.predict(X_test)\n",
        "elastic_netCV_rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "display(md(f'ElasticCV with $\\\\alpha$={elastic_netCV.alpha_}, $l1$ ratio={elastic_netCV.l1_ratio_}: ' +\n",
        "           f'{(generate_clean_model(elastic_netCV.intercept_, elastic_netCV.coef_, feature_names))}, ' +\n",
        "           f'RMSE: {elastic_netCV_rmse:.4f}, $R^2$: {elastic_netCV_r2:.4f}'))\n",
        "display(md(f'Original Data: {(generate_clean_model(bias_data, coef_data, feature_names))}'))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "ElasticCV with $\\alpha$=0.01, $l1$ ratio=1.0: $y = 5.190 + 39.511x_{2} + 16.112x_{4} + 34.599x_{5} + 0.005x_{6} + -0.012x_{8}$, RMSE: 2.0544, $R^2$: 0.9987",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Original Data: $y = 5.164 + 39.530x_{2} + 16.115x_{4} + 34.584x_{5}$",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ_mQ0KvjzUf"
      },
      "source": [
        "### ElasticNet with subsets of features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qir406pFMsQ5"
      },
      "source": [
        "ElasticNetCV's performance in this case is nearly identical to that of Lasso.Here, we test the power set of features to find the set of features, alpha, and l1_ratio values that generate the lowest RMSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4FMmp1Sj25p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "126cb7ba-83f1-4334-d95e-def9fe1ad8e9"
      },
      "source": [
        "# use power set\n",
        "\n",
        "df_elastic_net = pd.DataFrame(columns=['feature_set', 'alpha', 'rmse', 'r2'])\n",
        "\n",
        "for feature_set in feature_powerset:\n",
        "    for alpha in alphas:\n",
        "        elastic_net = Lasso(alpha=alpha, fit_intercept=True, random_state=random_state)\n",
        "        scores = cross_val_score(elastic_net, X_train[:,np.array(feature_set)], y_train, scoring='neg_root_mean_squared_error', cv=10)\n",
        "        elastic_net.fit(X_train[:,np.array(feature_set)], y_train)\n",
        "        df_elastic_net.loc[len(df_elastic_net.index)] = [','.join(str(e) for e in feature_set), alpha, -np.mean(scores), elastic_net.score(X_test[:,np.array(feature_set)], y_test)]\n",
        "\n",
        "best_elastic_net = {}\n",
        "best_elastic_net['params'] = df_elastic_net.loc[np.argmin(df_elastic_net['rmse'])]\n",
        "best_elastic_net['features'] = [int(e) for e in best_elastic_net['params']['feature_set'].split(',')]\n",
        "best_elastic_net['feature_names'] = feature_names[np.array(best_elastic_net['features'])]\n",
        "best_elastic_net['alpha'] = best_elastic_net['params']['alpha']\n",
        "\n",
        "best_elastic_net['model'] = Lasso(alpha=best_elastic_net['alpha'])\n",
        "best_elastic_net['model'].fit(X_train[:,np.array(best_elastic_net['features'])], y_train)\n",
        "best_elastic_net['r2'] = best_elastic_net['model'].score(X_test[:,np.array(best_elastic_net['features'])], y_test)\n",
        "y_pred = best_elastic_net['model'].predict(X_test[:,np.array(best_elastic_net['features'])])\n",
        "best_elastic_net['rmse'] = mean_squared_error(y_test, y_pred, squared=False)\n",
        "\n",
        "display(md(f'Best ElasticNet with $\\\\alpha$={best_elastic_net[\"alpha\"]}, $l1$ ratio={elastic_netCV.l1_ratio_}: ' +\n",
        "           f'{(generate_clean_model(best_elastic_net[\"model\"].intercept_, best_elastic_net[\"model\"].coef_, best_elastic_net[\"feature_names\"]))}, ' + \n",
        "           f'RMSE: {best_elastic_net[\"rmse\"]:.4f}, ' +\n",
        "           f'$R^2$: {best_elastic_net[\"r2\"]:.4f}'\n",
        "           ))\n",
        "display(md(f'Original Data: {(generate_clean_model(bias_data, coef_data, feature_names))}'))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Best ElasticNet with $\\alpha$=0.0001, $l1$ ratio=1.0: $y = 5.190 + 39.521x_{2} + 16.122x_{4} + 34.609x_{5}$, RMSE: 2.0540, $R^2$: 0.9987",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Original Data: $y = 5.164 + 39.530x_{2} + 16.115x_{4} + 34.584x_{5}$",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}