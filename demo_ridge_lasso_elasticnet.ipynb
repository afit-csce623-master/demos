{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "demo_ridge_lasso_elasticnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN67pzS9kaSx3amb3VvKtf1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afit-csce623-master/demos/blob/main/demo_ridge_lasso_elasticnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqRv7gnYXNHF"
      },
      "source": [
        "# Ridge, Lasso, and ElasticNet Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUcs5fH98-Lu"
      },
      "source": [
        "This notebook demonstrates the use of Ridge, Lasso, and ElasticNet on a linear regression dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8dnwK5nXSr2"
      },
      "source": [
        "## Generate Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ykyhqHJ-xYc"
      },
      "source": [
        "Here we generate regression data with 8 features, but only 3 features are needed to determine the target `y`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3XoZQtRM6hl"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "from IPython.display import Markdown as md\n",
        "from IPython.display import display, Math, Latex\n",
        "\n",
        "# initialize random state and regression parameters\n",
        "random_state = np.random.RandomState(101)\n",
        "n_features = 8\n",
        "n_informative = 3\n",
        "bias_data = random_state.uniform(0,10)\n",
        "noise = 2\n",
        "\n",
        "# Build a regression task\n",
        "X, y, coef_data = make_regression(n_samples=6000, \n",
        "                                  n_features=n_features, n_informative=n_informative, \n",
        "                                  bias=bias_data, noise=noise,\n",
        "                                  random_state=random_state, coef=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATRKQBA3XUkd"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQJREDcY-52E"
      },
      "source": [
        "The following helper functions will be used throughout this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz4DvDr2Xhw8"
      },
      "source": [
        "### Generate Model Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wffxOZIE-9SL"
      },
      "source": [
        "This function generates a model string of the form $y = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n$ where $n$ is the number of coefficients in `betas`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOZbGERCOnlE"
      },
      "source": [
        "def generate_model_function(beta0, betas, feature_names):\n",
        "    if np.isclose(beta0, 0):\n",
        "        model_function = f'$y = '\n",
        "    else:\n",
        "        model_function = f'$y = {beta0:.3f} + '\n",
        "\n",
        "    for beta, feature_name in zip(betas, feature_names):\n",
        "        model_function += f'{beta:.3f}{feature_name} + '\n",
        "    \n",
        "    return model_function[:-3] + '$'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GIccjVpXlli"
      },
      "source": [
        "### Clean Up Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taJNjfp7_bMA"
      },
      "source": [
        "Some models don't clean-up very small coefficients, leaving values looking like $ \\dots + 0.000 x_3 + \\dots $. This function will filter out coefficients that are near zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBC7nnL4Qg6R"
      },
      "source": [
        "def clean_up_model(coefs, names):\n",
        "    non_zero_coef_filter = np.logical_not(np.isclose(coefs, 0))\n",
        "    non_zero_coefs = coefs[non_zero_coef_filter]\n",
        "    non_zero_names = names[non_zero_coef_filter]\n",
        "\n",
        "    return non_zero_coefs, non_zero_names\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j8f2or6Xp5T"
      },
      "source": [
        "### Generate Clean Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbcGUnDb_y7n"
      },
      "source": [
        "This function combines the above two functions to return a cleaned-up model string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNEoRvPhf1EL"
      },
      "source": [
        "def generate_clean_model(beta0, coefs, names):\n",
        "    coefs_, names_ = clean_up_model(coefs, names)\n",
        "    return generate_model_function(beta0, coefs_, names_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ato9lwsaXs-_"
      },
      "source": [
        "### Powerset Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RarJVNi1_27H"
      },
      "source": [
        "This function generates the power set of a list, excluding the empty set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMnCU3E2rmQw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92ac6234-ffd5-4c5f-e22e-40c564007576"
      },
      "source": [
        "# powerset function\n",
        "\n",
        "from itertools import chain, combinations\n",
        "\n",
        "def powerset(iterable, j=1, k=999):\n",
        "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
        "    s = list(iterable)\n",
        "\n",
        "    # start with 1, as we don't need the empty set\n",
        "    p = list(chain.from_iterable(combinations(s, r) for r in range(1, len(s)+1)))\n",
        "    return [item for item in p if len(item) >= j and len(item) <= k]\n",
        "\n",
        "feature_powerset = powerset(range(n_features))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0,), (1,), (2,), (3,), (4,), (5,), (6,), (7,), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (3, 4), (3, 5), (3, 6), (3, 7), (4, 5), (4, 6), (4, 7), (5, 6), (5, 7), (6, 7), (0, 1, 2), (0, 1, 3), (0, 1, 4), (0, 1, 5), (0, 1, 6), (0, 1, 7), (0, 2, 3), (0, 2, 4), (0, 2, 5), (0, 2, 6), (0, 2, 7), (0, 3, 4), (0, 3, 5), (0, 3, 6), (0, 3, 7), (0, 4, 5), (0, 4, 6), (0, 4, 7), (0, 5, 6), (0, 5, 7), (0, 6, 7), (1, 2, 3), (1, 2, 4), (1, 2, 5), (1, 2, 6), (1, 2, 7), (1, 3, 4), (1, 3, 5), (1, 3, 6), (1, 3, 7), (1, 4, 5), (1, 4, 6), (1, 4, 7), (1, 5, 6), (1, 5, 7), (1, 6, 7), (2, 3, 4), (2, 3, 5), (2, 3, 6), (2, 3, 7), (2, 4, 5), (2, 4, 6), (2, 4, 7), (2, 5, 6), (2, 5, 7), (2, 6, 7), (3, 4, 5), (3, 4, 6), (3, 4, 7), (3, 5, 6), (3, 5, 7), (3, 6, 7), (4, 5, 6), (4, 5, 7), (4, 6, 7), (5, 6, 7), (0, 1, 2, 3), (0, 1, 2, 4), (0, 1, 2, 5), (0, 1, 2, 6), (0, 1, 2, 7), (0, 1, 3, 4), (0, 1, 3, 5), (0, 1, 3, 6), (0, 1, 3, 7), (0, 1, 4, 5), (0, 1, 4, 6), (0, 1, 4, 7), (0, 1, 5, 6), (0, 1, 5, 7), (0, 1, 6, 7), (0, 2, 3, 4), (0, 2, 3, 5), (0, 2, 3, 6), (0, 2, 3, 7), (0, 2, 4, 5), (0, 2, 4, 6), (0, 2, 4, 7), (0, 2, 5, 6), (0, 2, 5, 7), (0, 2, 6, 7), (0, 3, 4, 5), (0, 3, 4, 6), (0, 3, 4, 7), (0, 3, 5, 6), (0, 3, 5, 7), (0, 3, 6, 7), (0, 4, 5, 6), (0, 4, 5, 7), (0, 4, 6, 7), (0, 5, 6, 7), (1, 2, 3, 4), (1, 2, 3, 5), (1, 2, 3, 6), (1, 2, 3, 7), (1, 2, 4, 5), (1, 2, 4, 6), (1, 2, 4, 7), (1, 2, 5, 6), (1, 2, 5, 7), (1, 2, 6, 7), (1, 3, 4, 5), (1, 3, 4, 6), (1, 3, 4, 7), (1, 3, 5, 6), (1, 3, 5, 7), (1, 3, 6, 7), (1, 4, 5, 6), (1, 4, 5, 7), (1, 4, 6, 7), (1, 5, 6, 7), (2, 3, 4, 5), (2, 3, 4, 6), (2, 3, 4, 7), (2, 3, 5, 6), (2, 3, 5, 7), (2, 3, 6, 7), (2, 4, 5, 6), (2, 4, 5, 7), (2, 4, 6, 7), (2, 5, 6, 7), (3, 4, 5, 6), (3, 4, 5, 7), (3, 4, 6, 7), (3, 5, 6, 7), (4, 5, 6, 7), (0, 1, 2, 3, 4), (0, 1, 2, 3, 5), (0, 1, 2, 3, 6), (0, 1, 2, 3, 7), (0, 1, 2, 4, 5), (0, 1, 2, 4, 6), (0, 1, 2, 4, 7), (0, 1, 2, 5, 6), (0, 1, 2, 5, 7), (0, 1, 2, 6, 7), (0, 1, 3, 4, 5), (0, 1, 3, 4, 6), (0, 1, 3, 4, 7), (0, 1, 3, 5, 6), (0, 1, 3, 5, 7), (0, 1, 3, 6, 7), (0, 1, 4, 5, 6), (0, 1, 4, 5, 7), (0, 1, 4, 6, 7), (0, 1, 5, 6, 7), (0, 2, 3, 4, 5), (0, 2, 3, 4, 6), (0, 2, 3, 4, 7), (0, 2, 3, 5, 6), (0, 2, 3, 5, 7), (0, 2, 3, 6, 7), (0, 2, 4, 5, 6), (0, 2, 4, 5, 7), (0, 2, 4, 6, 7), (0, 2, 5, 6, 7), (0, 3, 4, 5, 6), (0, 3, 4, 5, 7), (0, 3, 4, 6, 7), (0, 3, 5, 6, 7), (0, 4, 5, 6, 7), (1, 2, 3, 4, 5), (1, 2, 3, 4, 6), (1, 2, 3, 4, 7), (1, 2, 3, 5, 6), (1, 2, 3, 5, 7), (1, 2, 3, 6, 7), (1, 2, 4, 5, 6), (1, 2, 4, 5, 7), (1, 2, 4, 6, 7), (1, 2, 5, 6, 7), (1, 3, 4, 5, 6), (1, 3, 4, 5, 7), (1, 3, 4, 6, 7), (1, 3, 5, 6, 7), (1, 4, 5, 6, 7), (2, 3, 4, 5, 6), (2, 3, 4, 5, 7), (2, 3, 4, 6, 7), (2, 3, 5, 6, 7), (2, 4, 5, 6, 7), (3, 4, 5, 6, 7), (0, 1, 2, 3, 4, 5), (0, 1, 2, 3, 4, 6), (0, 1, 2, 3, 4, 7), (0, 1, 2, 3, 5, 6), (0, 1, 2, 3, 5, 7), (0, 1, 2, 3, 6, 7), (0, 1, 2, 4, 5, 6), (0, 1, 2, 4, 5, 7), (0, 1, 2, 4, 6, 7), (0, 1, 2, 5, 6, 7), (0, 1, 3, 4, 5, 6), (0, 1, 3, 4, 5, 7), (0, 1, 3, 4, 6, 7), (0, 1, 3, 5, 6, 7), (0, 1, 4, 5, 6, 7), (0, 2, 3, 4, 5, 6), (0, 2, 3, 4, 5, 7), (0, 2, 3, 4, 6, 7), (0, 2, 3, 5, 6, 7), (0, 2, 4, 5, 6, 7), (0, 3, 4, 5, 6, 7), (1, 2, 3, 4, 5, 6), (1, 2, 3, 4, 5, 7), (1, 2, 3, 4, 6, 7), (1, 2, 3, 5, 6, 7), (1, 2, 4, 5, 6, 7), (1, 3, 4, 5, 6, 7), (2, 3, 4, 5, 6, 7), (0, 1, 2, 3, 4, 5, 6), (0, 1, 2, 3, 4, 5, 7), (0, 1, 2, 3, 4, 6, 7), (0, 1, 2, 3, 5, 6, 7), (0, 1, 2, 4, 5, 6, 7), (0, 1, 3, 4, 5, 6, 7), (0, 2, 3, 4, 5, 6, 7), (1, 2, 3, 4, 5, 6, 7), (0, 1, 2, 3, 4, 5, 6, 7)]\n",
            "[(0,), (1,), (2,), (3,), (4,), (5,), (6,), (7,), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (3, 4), (3, 5), (3, 6), (3, 7), (4, 5), (4, 6), (4, 7), (5, 6), (5, 7), (6, 7)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tmXjkZyAAqn"
      },
      "source": [
        "## Prepatory actons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6OBz3oiAIma"
      },
      "source": [
        "Here, we split the train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuphFhLhNZfd"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split and sequester test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=random_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTjhNsAWAOMM"
      },
      "source": [
        "We set some variables that we will reuse throught the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoEsu_RLNRgt"
      },
      "source": [
        "kfold_size = 10\n",
        "\n",
        "# create feature names -- these are generic feature names x_1, x_2, ..., x_25\n",
        "feature_names = np.array([f'x_{{{idx+1}}}' for idx in range(n_features)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58IQ3r_9TExT"
      },
      "source": [
        "## Ridge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mYuY_ayYxpo"
      },
      "source": [
        "### Set alpha values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aclnC-OAIum5"
      },
      "source": [
        "We will set alpha values. With Ridge regression, we will include 0.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvJ5dcFJYzD3"
      },
      "source": [
        "alpha_min = 0.0000001\n",
        "alpha_max = 100000\n",
        "alpha_num = int(abs(np.log10(alpha_min)) + abs(np.log10(alpha_max)) + 1)\n",
        "alphas = np.geomspace(alpha_min, alpha_max, num=alpha_num)\n",
        "alphas = np.insert(alphas, 0, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCrdAxa0Yhry"
      },
      "source": [
        "### Ridge - manual cross validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk2x6oTDI46y"
      },
      "source": [
        "The following code uses the Ridge regression model for each alpha value. We keep track of each RMSE, and select the model that yielded the lowest RMSE. We compare the model with the model that generated the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "ObNmaKPxWQBs",
        "outputId": "7b26e73d-2e9d-4971-f3fa-dd13ac03d81e"
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "df_ridge = pd.DataFrame(columns=['alpha', 'rmse', 'r2'])\n",
        "\n",
        "for alpha in alphas:\n",
        "    ridge = Ridge(alpha=alpha, fit_intercept=True, random_state=random_state)\n",
        "    scores = cross_val_score(ridge, X_train, y_train, scoring='neg_root_mean_squared_error', cv=kfold_size)\n",
        "    ridge.fit(X_train, y_train)\n",
        "    df_ridge.loc[len(df_ridge.index)] = [alpha, -np.mean(scores), ridge.score(X_test, y_test)]\n",
        "\n",
        "best_ridge = {}\n",
        "best_ridge['alpha'] = df_ridge.loc[np.argmin(df_ridge['rmse'])]['alpha']\n",
        "best_ridge['model'] = Ridge(alpha=best_ridge['alpha'], fit_intercept=True, random_state=random_state)\n",
        "best_ridge['model'].fit(X_train, y_train)\n",
        "best_ridge['r2'] = best_ridge['model'].score(X_test, y_test)\n",
        "y_pred = best_ridge['model'].predict(X_test)\n",
        "best_ridge['rmse'] = mean_squared_error(y_test, y_pred, squared=False)\n",
        "\n",
        "display(md(f'Best Ridge with $\\\\alpha$={best_ridge[\"alpha\"]}: ' +\n",
        "           f'{(generate_clean_model(best_ridge[\"model\"].intercept_, best_ridge[\"model\"].coef_, feature_names))}, ' +\n",
        "           f'RMSE: {best_ridge[\"rmse\"]:.4f}, $R^2$: {best_ridge[\"r2\"]:.4f}'))\n",
        "display(md(f'Original Data: {(generate_clean_model(bias_data, coef_data, feature_names))}'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Best Ridge with $\\alpha$=0.0: $y = 5.190 + 0.004x_{1} + 39.521x_{2} + -0.001x_{3} + 16.122x_{4} + 34.609x_{5} + 0.015x_{6} + -0.001x_{7} + -0.023x_{8}$, RMSE: 2.0544, $R^2$: 0.9987",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Original Data: $y = 5.164 + 39.530x_{2} + 16.115x_{4} + 34.584x_{5}$",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdQgNT4wYlDo"
      },
      "source": [
        "### RidgeCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0CclZFwJNKV"
      },
      "source": [
        "The following code uses the RidgeCV to automatically find the alpha value that yields the smallest RMSE. We compare the model with the model that generated the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "id": "bWx9570fYm-E",
        "outputId": "a6cab95a-8112-4a62-af67-4acc1c1dc24d"
      },
      "source": [
        "from sklearn.linear_model import RidgeCV\n",
        "\n",
        "ridgeCV = RidgeCV(alphas=alphas,\n",
        "                  scoring='neg_root_mean_squared_error',\n",
        "                  cv=kfold_size)\n",
        "ridgeCV.fit(X_train, y_train)\n",
        "ridgeCV_r2 = ridgeCV.score(X_test, y_test)\n",
        "y_pred = ridgeCV.predict(X_test)\n",
        "ridgeCV_rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "display(md(f'RidgeCV with $\\\\alpha$={ridgeCV.alpha_}: ' +\n",
        "           f'{(generate_clean_model(ridgeCV.intercept_, ridgeCV.coef_, feature_names))}, ' +\n",
        "           f'RMSE: {ridgeCV_rmse:.4f}, $R^2$: {ridgeCV_r2:.4f}'))\n",
        "display(md(f'Original Data: {(generate_clean_model(bias_data, coef_data, feature_names))}'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "RidgeCV with $\\alpha$=0.0: $y = 5.190 + 0.004x_{1} + 39.521x_{2} + -0.001x_{3} + 16.122x_{4} + 34.609x_{5} + 0.015x_{6} + -0.001x_{7} + -0.023x_{8}$, RMSE: 2.0544, $R^2$: 0.9987",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Original Data: $y = 5.164 + 39.530x_{2} + 16.115x_{4} + 34.584x_{5}$",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGVfIW4ubks2"
      },
      "source": [
        "### Ridge with subsets of features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N75D307tJWaR"
      },
      "source": [
        "Neither Ridge nor RidgeCV is able to fully eliminate unimportant features. Here, we test the power set of features to find the set of features and alpha value that generates the lowest RMSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "id": "eiP13kfDbnF_",
        "outputId": "49a1180a-56a5-45c0-b556-82b1756136f6"
      },
      "source": [
        "# use power set\n",
        "import time\n",
        "\n",
        "df_ridge = pd.DataFrame(columns=['feature_set', 'alpha', 'rmse', 'r2'])\n",
        "\n",
        "trial_time = time.perf_counter()\n",
        "start_time = time.perf_counter()\n",
        "best_rmse = 1000\n",
        "for idx, feature_set in enumerate(feature_powerset):\n",
        "    feature_set_str = ','.join(str(e) for e in feature_set)\n",
        "    feature_set_name_str = ','.join(str(e) for e in feature_names[np.array(feature_set)])\n",
        "    for alpha in alphas:\n",
        "        ridge = Ridge(alpha=alpha, fit_intercept=True, random_state=random_state)\n",
        "        scores = cross_val_score(ridge, X_train[:,np.array(feature_set)], y_train, scoring='neg_root_mean_squared_error', cv=10)\n",
        "        ridge.fit(X_train[:,np.array(feature_set)], y_train)\n",
        "        df_ridge.loc[len(df_ridge.index)] = [feature_set_str, alpha, -np.mean(scores), ridge.score(X_test[:,np.array(feature_set)], y_test)]\n",
        "\n",
        "        # print to show progress\n",
        "        if (-np.mean(scores) < best_rmse):\n",
        "            print('\\r', f'{time.perf_counter() - start_time:.2f}s {time.perf_counter() - trial_time:.2f}s {idx} of {len(feature_powerset)}: {feature_set_name_str} {-np.mean(scores)}')\n",
        "            best_rmse = -np.mean(scores)\n",
        "        else:\n",
        "            print('\\r', f'{time.perf_counter() - start_time:.2f}s {time.perf_counter() - trial_time:.2f}s {idx} of {len(feature_powerset)}: {feature_set_name_str} {-np.mean(scores)}', end='')\n",
        "        trial_time = time.perf_counter()\n",
        "        \n",
        "print(f'Total time: {time.perf_counter() - start_time}')\n",
        "\n",
        "best_ridge = {}\n",
        "best_ridge['params'] = df_ridge.loc[np.argmin(df_ridge['rmse'])]\n",
        "best_ridge['features'] = [int(e) for e in best_ridge['params']['feature_set'].split(',')]\n",
        "best_ridge['feature_names'] = feature_names[np.array(best_ridge['features'])]\n",
        "best_ridge['alpha'] = best_ridge['params']['alpha']\n",
        "\n",
        "best_ridge['model'] = Ridge(alpha=best_ridge['alpha'])\n",
        "best_ridge['model'].fit(X_train[:,np.array(best_ridge['features'])], y_train)\n",
        "best_ridge['r2'] = best_ridge['model'].score(X_test[:,np.array(best_ridge['features'])], y_test)\n",
        "y_pred = best_ridge['model'].predict(X_test[:,np.array(best_ridge['features'])])\n",
        "best_ridge['rmse'] = mean_squared_error(y_test, y_pred, squared=False)\n",
        "\n",
        "display(md(f'Best Ridge with $\\\\alpha$={best_ridge[\"alpha\"]}: ' +\n",
        "           f'{(generate_clean_model(best_ridge[\"model\"].intercept_, best_ridge[\"model\"].coef_, best_ridge[\"feature_names\"]))}, ' + \n",
        "           f'RMSE: {best_ridge[\"rmse\"]:.4f}, ' +\n",
        "           f'$R^2$: {best_ridge[\"r2\"]:.4f}'\n",
        "           ))\n",
        "display(md(f'Original Data: {(generate_clean_model(bias_data, coef_data, feature_names))}'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r 0.02s 0.02s 0 of 36: x_{1} 54.894305797170375\n",
            "\r 0.04s 0.02s 0 of 36: x_{1} 54.89430579716848\n",
            "\r 0.07s 0.02s 0 of 36: x_{1} 54.89430579714944\n",
            "\r 0.09s 0.02s 0 of 36: x_{1} 54.89430579695919\n",
            "\r 0.12s 0.02s 0 of 36: x_{1} 54.89430579505655\n",
            "\r 0.14s 0.03s 0 of 36: x_{1} 54.894305776030265\n",
            "\r 0.17s 0.02s 0 of 36: x_{1} 54.89430558577236\n",
            "\r 0.19s 0.03s 0 of 36: x_{1} 54.89430368369953\n",
            " 0.21s 0.02s 0 of 36: x_{1} 54.89428471345144\n",
            " 0.24s 0.03s 0 of 36: x_{1} 54.89409992895886\n",
            " 0.27s 0.02s 0 of 36: x_{1} 54.89264084305279\n",
            " 0.29s 0.02s 0 of 36: x_{1} 54.8886312559471\n",
            " 0.31s 0.02s 0 of 36: x_{1} 54.88686596200322\n",
            " 0.33s 0.02s 1 of 36: x_{2} 38.51465632607695\n",
            " 0.36s 0.02s 1 of 36: x_{2} 38.51465632607678\n",
            " 0.38s 0.02s 1 of 36: x_{2} 38.51465632607514\n",
            " 0.40s 0.02s 1 of 36: x_{2} 38.51465632605861\n",
            " 0.43s 0.02s 1 of 36: x_{2} 38.51465632589443\n",
            " 0.45s 0.03s 1 of 36: x_{2} 38.51465632435441\n",
            " 0.48s 0.03s 1 of 36: x_{2} 38.5146563191349\n",
            " 5.34s 0.03s 16 of 36: x_{2},x_{4} 34.99425450398118\n",
            " 5.37s 0.03s 16 of 36: x_{2},x_{4} 34.99425450397539\n",
            " 5.39s 0.03s 16 of 36: x_{2},x_{4} 34.994254503917446\n",
            " 5.43s 0.03s 16 of 36: x_{2},x_{4} 34.9942545033379\n",
            " 5.46s 0.03s 16 of 36: x_{2},x_{4} 34.99425449754385\n",
            " 5.49s 0.03s 16 of 36: x_{2},x_{4} 34.994254439736565\n",
            " 5.52s 0.03s 16 of 36: x_{2},x_{4} 34.994253874976515\n",
            " 5.54s 0.03s 16 of 36: x_{2},x_{4} 34.994249558024094\n",
            " 5.71s 0.03s 17 of 36: x_{2},x_{5} 16.226173079661244\n",
            " 5.73s 0.03s 17 of 36: x_{2},x_{5} 16.22617307965252\n",
            " 5.77s 0.03s 17 of 36: x_{2},x_{5} 16.226173079565292\n",
            " 5.79s 0.03s 17 of 36: x_{2},x_{5} 16.226173078693062\n",
            " 5.82s 0.03s 17 of 36: x_{2},x_{5} 16.226173069974777\n",
            " 5.85s 0.03s 17 of 36: x_{2},x_{5} 16.22617298319471\n",
            " 5.87s 0.03s 17 of 36: x_{2},x_{5} 16.226172155669364\n",
            " 5.91s 0.03s 17 of 36: x_{2},x_{5} 16.226167906098762\n",
            " 12.29s 0.02s 35 of 36: x_{7},x_{8} 54.886925196983Total time: 12.28847057800067\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Best Ridge with $\\alpha$=1.0: $y = 5.004 + 39.236x_{2} + 34.596x_{5}$, RMSE: 16.4833, $R^2$: 0.9132",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Original Data: $y = 5.164 + 39.530x_{2} + 16.115x_{4} + 34.584x_{5}$",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81tbIKecZErl"
      },
      "source": [
        "## Lasso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qihdwBP9ZHV_"
      },
      "source": [
        "### Set alpha values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjTE1niELOI_"
      },
      "source": [
        "We will set alpha values. With Lasso regression, we will not include 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fltmQvKZWBl"
      },
      "source": [
        "alpha_min = 0.0000001\n",
        "alpha_max = 100000\n",
        "alpha_num = int(abs(np.log10(alpha_min)) + abs(np.log10(alpha_max)) + 1)\n",
        "alphas = np.geomspace(alpha_min, alpha_max, num=alpha_num)\n",
        "\n",
        "# Lasso is unreliable with alpha=0, use LinearRegression instead when alpha=0 is desired\n",
        "#alphas = np.insert(alphas, 0, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFgXQuwfZLlm"
      },
      "source": [
        "### Lasso - manual cross validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0gTkM7iLSl-"
      },
      "source": [
        "The following code uses the Lasso regression model for each alpha value. We keep track of each RMSE, and select the model that yielded the lowest RMSE. We compare the model with the model that generated the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "jka9W-bxZf-V",
        "outputId": "e8f7405e-cd9d-4470-c879-588e9b9d34fd"
      },
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "df_lasso = pd.DataFrame(columns=['alpha', 'rmse', 'r2'])\n",
        "\n",
        "for alpha in alphas:\n",
        "    lasso = Lasso(alpha=alpha, fit_intercept=True, random_state=random_state)\n",
        "    scores = cross_val_score(lasso, X_train, y_train, scoring='neg_root_mean_squared_error', cv=kfold_size)\n",
        "    lasso.fit(X_train, y_train)\n",
        "    df_lasso.loc[len(df_lasso.index)] = [alpha, -np.mean(scores), lasso.score(X_test, y_test)]\n",
        "\n",
        "best_lasso = {}\n",
        "best_lasso['alpha'] = df_lasso.loc[np.argmin(df_lasso['rmse'])]['alpha']\n",
        "best_lasso['model'] = Lasso(alpha=best_lasso['alpha'], fit_intercept=True, random_state=random_state)\n",
        "best_lasso['model'].fit(X_train, y_train)\n",
        "best_lasso['r2'] = best_lasso['model'].score(X_test, y_test)\n",
        "y_pred = best_lasso['model'].predict(X_test)\n",
        "best_lasso['rmse'] = mean_squared_error(y_test, y_pred, squared=False)\n",
        "\n",
        "display(md(f'Best Ridge with $\\\\alpha$={best_lasso[\"alpha\"]}: ' +\n",
        "           f'{(generate_clean_model(best_lasso[\"model\"].intercept_, best_lasso[\"model\"].coef_, feature_names))}, ' +\n",
        "           f'RMSE: {best_lasso[\"rmse\"]:.4f}, $R^2$: {best_lasso[\"r2\"]:.4f}'))\n",
        "display(md(f'Original Data: {(generate_clean_model(bias_data, coef_data, feature_names))}'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Best Ridge with $\\alpha$=0.01: $y = 5.190 + 39.511x_{2} + 16.112x_{4} + 34.599x_{5} + 0.005x_{6} + -0.012x_{8}$, RMSE: 2.0544, $R^2$: 0.9987",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Original Data: $y = 5.164 + 39.530x_{2} + 16.115x_{4} + 34.584x_{5}$",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwVSU7dEZOhF"
      },
      "source": [
        "### LassoCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMVm5z1SLYwY"
      },
      "source": [
        "The following code uses the LassoCV to automatically find the alpha value that yields the smallest RMSE. We compare the model with the model that generated the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "-UP8k_u3ZGWE",
        "outputId": "97538020-a349-42e7-97f5-569290293e31"
      },
      "source": [
        "from sklearn.linear_model import LassoCV\n",
        "\n",
        "lassoCV = LassoCV(alphas=alphas,\n",
        "                #   scoring='neg_root_mean_squared_error',\n",
        "                  cv=kfold_size)\n",
        "lassoCV.fit(X_train, y_train)\n",
        "lassoCV_r2 = lassoCV.score(X_test, y_test)\n",
        "y_pred = lassoCV.predict(X_test)\n",
        "lassoCV_rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "display(md(f'LassoCV with $\\\\alpha$={lassoCV.alpha_}: ' +\n",
        "           f'{(generate_clean_model(lassoCV.intercept_, lassoCV.coef_, feature_names))}, ' +\n",
        "           f'RMSE: {lassoCV_rmse:.4f}, $R^2$: {lassoCV_r2:.4f}'))\n",
        "display(md(f'Original Data: {(generate_clean_model(bias_data, coef_data, feature_names))}'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "LassoCV with $\\alpha$=0.01: $y = 5.190 + 39.511x_{2} + 16.112x_{4} + 34.599x_{5} + 0.005x_{6} + -0.012x_{8}$, RMSE: 2.0544, $R^2$: 0.9987",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Original Data: $y = 5.164 + 39.530x_{2} + 16.115x_{4} + 34.584x_{5}$",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZobiGFXhcIVi"
      },
      "source": [
        "### Lasso with subsets of features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnkfxZvPLnGd"
      },
      "source": [
        "Though it performs better than Ridge, neither Lasso nor LassoCV is able to fully eliminate unimportant features. Here, we test the power set of features to find the set of features and alpha value that generates the lowest RMSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "RG91g4QsDrdl",
        "outputId": "f3ac1464-f254-4714-9544-a802fbc9b91e"
      },
      "source": [
        "# use power set\n",
        "\n",
        "df_lasso = pd.DataFrame(columns=['feature_set', 'alpha', 'rmse', 'r2'])\n",
        "\n",
        "for feature_set in feature_powerset:\n",
        "    for alpha in alphas:\n",
        "        lasso = Lasso(alpha=alpha, fit_intercept=True, random_state=random_state)\n",
        "        scores = cross_val_score(lasso, X_train[:,np.array(feature_set)], y_train, scoring='neg_root_mean_squared_error', cv=10)\n",
        "        lasso.fit(X_train[:,np.array(feature_set)], y_train)\n",
        "        df_lasso.loc[len(df_lasso.index)] = [','.join(str(e) for e in feature_set), alpha, -np.mean(scores), lasso.score(X_test[:,np.array(feature_set)], y_test)]\n",
        "\n",
        "best_lasso = {}\n",
        "best_lasso['params'] = df_lasso.loc[np.argmin(df_lasso['rmse'])]\n",
        "best_lasso['features'] = [int(e) for e in best_lasso['params']['feature_set'].split(',')]\n",
        "best_lasso['feature_names'] = feature_names[np.array(best_lasso['features'])]\n",
        "best_lasso['alpha'] = best_lasso['params']['alpha']\n",
        "\n",
        "best_lasso['model'] = Lasso(alpha=best_lasso['alpha'])\n",
        "best_lasso['model'].fit(X_train[:,np.array(best_lasso['features'])], y_train)\n",
        "best_lasso['r2'] = best_lasso['model'].score(X_test[:,np.array(best_lasso['features'])], y_test)\n",
        "y_pred = best_lasso['model'].predict(X_test[:,np.array(best_lasso['features'])])\n",
        "best_lasso['rmse'] = mean_squared_error(y_test, y_pred, squared=False)\n",
        "\n",
        "display(md(f'Best Lasso with $\\\\alpha$={best_lasso[\"alpha\"]}: ' +\n",
        "           f'{(generate_clean_model(best_lasso[\"model\"].intercept_, best_lasso[\"model\"].coef_, best_lasso[\"feature_names\"]))}, ' + \n",
        "           f'RMSE: {best_lasso[\"rmse\"]:.4f}, ' +\n",
        "           f'$R^2$: {best_lasso[\"r2\"]:.4f}'\n",
        "           ))\n",
        "display(md(f'Original Data: {(generate_clean_model(bias_data, coef_data, feature_names))}'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-7bb9b5ebf8a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mlasso\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLasso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlasso\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg_root_mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mlasso\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mdf_lasso\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_lasso\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlasso\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    388\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    391\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 236\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m_safe_split\u001b[0;34m(estimator, X, y, indices, train_indices)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mX_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mX_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_pandas_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_array_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_list_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_array_indexing\u001b[0;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7zE5eoVeDzX"
      },
      "source": [
        "## ElasticNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b6kgVtaeaLT"
      },
      "source": [
        "### Set alpha values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNeSzHUTLw4I"
      },
      "source": [
        "We will set alpha values. With ElasticNet regression, which includes Lasso when l1_ratio=1, we will not include 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nUnghLXeb7e"
      },
      "source": [
        "alpha_min = 0.0000001\n",
        "alpha_max = 100000\n",
        "alpha_num = int(abs(np.log10(alpha_min)) + abs(np.log10(alpha_max)) + 1)\n",
        "alphas = np.geomspace(alpha_min, alpha_max, num=alpha_num)\n",
        "\n",
        "# Lasso is unreliable with alpha=0, use LinearRegression instead when alpha=0 is desired\n",
        "#alphas = np.insert(alphas, 0, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU6kNhXwekAb"
      },
      "source": [
        "### Set lr_ratio values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuDJQsPhME9R"
      },
      "source": [
        "Set l1_ratio values. This is that amount of l1 regularization the linear model will attempt to perform (l1_ratio = 1.0 corresponds with Lasso, and l1_ratio = 0.0 corresponds with Ridge)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyT63444eGle"
      },
      "source": [
        "l1_ratios = np.linspace(0.01, 1.0 ,num=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ztjUcEvjwTx"
      },
      "source": [
        "### ElasticNetCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlK4GqaGMDh2"
      },
      "source": [
        "The following code uses the ElasticNetCV to automatically find the alpha and l1_ratio values that yield the smallest RMSE. We compare the model with the model that generated the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "K7nDCe_Vf29r",
        "outputId": "14d018aa-80ad-4373-f4df-250b3556ba1b"
      },
      "source": [
        "from sklearn.linear_model import ElasticNetCV\n",
        "\n",
        "elastic_netCV = ElasticNetCV(alphas=alphas,\n",
        "                      l1_ratio = l1_ratios,\n",
        "                #   scoring='neg_root_mean_squared_error',\n",
        "                  cv=kfold_size,\n",
        "                  max_iter=1000)\n",
        "elastic_netCV.fit(X_train, y_train)\n",
        "elastic_netCV_r2 = elastic_netCV.score(X_test, y_test)\n",
        "y_pred = elastic_netCV.predict(X_test)\n",
        "elastic_netCV_rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "display(md(f'ElasticCV with $\\\\alpha$={elastic_netCV.alpha_}, $l1$ ratio={elastic_netCV.l1_ratio_}: ' +\n",
        "           f'{(generate_clean_model(elastic_netCV.intercept_, elastic_netCV.coef_, feature_names))}, ' +\n",
        "           f'RMSE: {elastic_netCV_rmse:.4f}, $R^2$: {elastic_netCV_r2:.4f}'))\n",
        "display(md(f'Original Data: {(generate_clean_model(bias_data, coef_data, feature_names))}'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "ElasticCV with $\\alpha$=0.01, $l1$ ratio=1.0: $y = 5.190 + 39.511x_{2} + 16.112x_{4} + 34.599x_{5} + 0.005x_{6} + -0.012x_{8}$, RMSE: 2.0544, $R^2$: 0.9987",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Original Data: $y = 5.164 + 39.530x_{2} + 16.115x_{4} + 34.584x_{5}$",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ_mQ0KvjzUf"
      },
      "source": [
        "### ElasticNet with subsets of features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qir406pFMsQ5"
      },
      "source": [
        "ElasticNetCV's performance in this case is nearly identical to that of Lasso.Here, we test the power set of features to find the set of features, alpha, and l1_ratio values that generate the lowest RMSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4FMmp1Sj25p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "dc7673ef-c13f-43f5-aeb3-c35577e523ad"
      },
      "source": [
        "# use power set\n",
        "\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "display(X_train)\n",
        "df_elastic_net = pd.DataFrame(columns=['feature_set', 'alpha', 'rmse', 'r2'])\n",
        "\n",
        "for feature_set in feature_powerset:\n",
        "    for alpha in alphas:\n",
        "        elastic_net = ElasticNet(alpha=alpha, fit_intercept=True, random_state=random_state)\n",
        "        scores = cross_val_score(elastic_net, X_train[:,np.array(feature_set)], y_train, scoring='neg_root_mean_squared_error', cv=10)\n",
        "        elastic_net.fit(X_train[:,np.array(feature_set)], y_train)\n",
        "        df_elastic_net.loc[len(df_elastic_net.index)] = [','.join(str(e) for e in feature_set), alpha, -np.mean(scores), elastic_net.score(X_test[:,np.array(feature_set)], y_test)]\n",
        "\n",
        "best_elastic_net = {}\n",
        "best_elastic_net['params'] = df_elastic_net.loc[np.argmin(df_elastic_net['rmse'])]\n",
        "best_elastic_net['features'] = [int(e) for e in best_elastic_net['params']['feature_set'].split(',')]\n",
        "best_elastic_net['feature_names'] = feature_names[np.array(best_elastic_net['features'])]\n",
        "best_elastic_net['alpha'] = best_elastic_net['params']['alpha']\n",
        "\n",
        "best_elastic_net['model'] = ElasticNet(alpha=best_elastic_net['alpha'])\n",
        "best_elastic_net['model'].fit(X_train[:,np.array(best_elastic_net['features'])], y_train)\n",
        "best_elastic_net['r2'] = best_elastic_net['model'].score(X_test[:,np.array(best_elastic_net['features'])], y_test)\n",
        "y_pred = best_elastic_net['model'].predict(X_test[:,np.array(best_elastic_net['features'])])\n",
        "best_elastic_net['rmse'] = mean_squared_error(y_test, y_pred, squared=False)\n",
        "\n",
        "display(md(f'Best ElasticNet with $\\\\alpha$={best_elastic_net[\"alpha\"]}, $l1$ ratio={elastic_netCV.l1_ratio_}: ' +\n",
        "           f'{(generate_clean_model(best_elastic_net[\"model\"].intercept_, best_elastic_net[\"model\"].coef_, best_elastic_net[\"feature_names\"]))}, ' + \n",
        "           f'RMSE: {best_elastic_net[\"rmse\"]:.4f}, ' +\n",
        "           f'$R^2$: {best_elastic_net[\"r2\"]:.4f}'\n",
        "           ))\n",
        "display(md(f'Original Data: {(generate_clean_model(bias_data, coef_data, feature_names))}'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[ 0.23432275, -0.25116476, -1.80481385, ..., -2.14277052,\n",
              "        -0.72729112, -1.10840967],\n",
              "       [ 0.58296477,  0.96021311,  1.80756321, ...,  0.19941368,\n",
              "        -2.56669041,  0.65102777],\n",
              "       [-0.47960683,  1.02398443,  1.02110385, ..., -0.50268084,\n",
              "        -1.14804132,  1.07161118],\n",
              "       ...,\n",
              "       [-0.51227324, -0.04435605,  0.58533465, ...,  1.13579355,\n",
              "        -1.62183302,  1.55792643],\n",
              "       [ 0.06461535, -0.12768327,  0.209688  , ...,  0.35447025,\n",
              "        -0.26766611, -0.49705465],\n",
              "       [-1.14075907, -0.0589376 ,  0.59474718, ..., -0.0843277 ,\n",
              "         0.61057984, -0.60238464]])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Best ElasticNet with $\\alpha$=1e-07, $l1$ ratio=1.0: $y = 5.190 + 39.521x_{2} + 16.122x_{4} + 34.609x_{5}$, RMSE: 2.0540, $R^2$: 0.9987",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Original Data: $y = 5.164 + 39.530x_{2} + 16.115x_{4} + 34.584x_{5}$",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}